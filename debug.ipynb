{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video:(16, 360, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/openclip/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1568, 768]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > indices[-1]:\n",
    "            break\n",
    "        if i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def sample_frame_indices(clip_len, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video evenly across the entire length.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        seg_len (`int`): Total number of frames in the video.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    indices = np.linspace(0, seg_len - 1, num=clip_len, endpoint=True)\n",
    "    indices = np.round(indices).astype(np.int64)\n",
    "    return indices.tolist()\n",
    "\n",
    "# def read_video_pyav(container, indices):\n",
    "#     '''\n",
    "#     Decode the video with PyAV decoder.\n",
    "#     Args:\n",
    "#         container (`av.container.input.InputContainer`): PyAV container.\n",
    "#         indices (`List[int]`): List of frame indices to decode.\n",
    "#     Returns:\n",
    "#         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "#     '''\n",
    "#     frames = []\n",
    "#     container.seek(0)\n",
    "#     start_index = indices[0]\n",
    "#     end_index = indices[-1]\n",
    "#     for i, frame in enumerate(container.decode(video=0)):\n",
    "#         if i > end_index:\n",
    "#             break\n",
    "#         if i >= start_index and i in indices:\n",
    "#             frames.append(frame)\n",
    "#     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "# def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "#     '''\n",
    "#     Sample a given number of frame indices from the video.\n",
    "#     Args:\n",
    "#         clip_len (`int`): Total number of frames to sample.\n",
    "#         frame_sample_rate (`int`): Sample every n-th frame.\n",
    "#         seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "#     Returns:\n",
    "#         indices (`List[int]`): List of sampled frame indices\n",
    "#     '''\n",
    "#     converted_len = int(clip_len * frame_sample_rate)\n",
    "#     end_idx = np.random.randint(converted_len, seg_len)\n",
    "#     start_idx = end_idx - converted_len\n",
    "#     indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "#     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "#     return indices\n",
    "\n",
    "\n",
    "# video clip consists of 300 frames (10 seconds at 30 FPS)\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    ")\n",
    "container = av.open(file_path)\n",
    "\n",
    "# sample 16 frames\n",
    "# indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "indices = sample_frame_indices(clip_len=16, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)\n",
    "print(f'video:{video.shape}')\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.8585, -0.6821,  0.0243,  ...,  0.1413,  0.6469, -0.0817],\n",
      "         [-0.1544,  0.0288, -0.0167,  ..., -0.3857,  0.1204, -0.1545],\n",
      "         [ 0.5996, -0.1285,  0.0552,  ...,  0.1270,  0.1242, -0.3623],\n",
      "         ...,\n",
      "         [ 0.6294, -0.0396, -0.0694,  ...,  0.4068, -0.1223,  0.0167],\n",
      "         [ 0.4512, -0.2887,  0.0535,  ...,  0.2846, -0.3203,  0.0321],\n",
      "         [ 0.6574, -0.0593, -0.0391,  ...,  0.4710, -0.1633, -0.1031]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(inputs['pixel_values'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[[-1.0048, -1.0048, -1.0219,  ..., -1.7583, -1.7069, -1.7069],\n",
      "           [-1.0219, -1.0219, -1.0390,  ..., -1.7583, -1.7069, -1.7069],\n",
      "           [-1.0562, -1.0562, -1.0562,  ..., -1.7583, -1.7069, -1.7069],\n",
      "           ...,\n",
      "           [-1.9638, -1.9638, -1.9467,  ...,  0.1254,  0.1083,  0.1083],\n",
      "           [-1.9980, -1.9980, -1.9809,  ...,  0.1254,  0.1083,  0.0912],\n",
      "           [-2.0494, -2.0494, -1.9980,  ...,  0.1083,  0.0912,  0.0741]],\n",
      "\n",
      "          [[-1.7556, -1.7556, -1.7556,  ..., -1.8606, -1.8081, -1.8081],\n",
      "           [-1.7731, -1.7731, -1.7731,  ..., -1.8606, -1.8081, -1.8081],\n",
      "           [-1.8081, -1.8081, -1.8081,  ..., -1.8606, -1.8081, -1.8081],\n",
      "           ...,\n",
      "           [-2.0357, -2.0357, -2.0357,  ..., -0.4951, -0.4951, -0.4951],\n",
      "           [-2.0357, -2.0357, -2.0357,  ..., -0.5126, -0.5126, -0.4951],\n",
      "           [-2.0357, -2.0357, -2.0357,  ..., -0.5126, -0.4951, -0.4776]],\n",
      "\n",
      "          [[-1.7173, -1.7173, -1.7173,  ..., -1.6824, -1.6302, -1.6302],\n",
      "           [-1.7347, -1.7347, -1.7347,  ..., -1.6824, -1.6302, -1.6302],\n",
      "           [-1.7696, -1.7696, -1.7696,  ..., -1.6824, -1.6302, -1.6302],\n",
      "           ...,\n",
      "           [-1.6999, -1.6999, -1.6824,  ..., -0.8981, -0.9330, -0.9678],\n",
      "           [-1.7347, -1.7347, -1.7173,  ..., -0.8458, -0.9156, -0.9504],\n",
      "           [-1.7870, -1.7870, -1.7173,  ..., -0.8458, -0.9156, -0.9330]]],\n",
      "\n",
      "\n",
      "         [[[-1.2959, -1.2445, -1.0048,  ..., -1.9295, -1.9124, -1.8953],\n",
      "           [-1.5014, -1.4500, -1.2959,  ..., -1.9295, -1.9124, -1.8953],\n",
      "           [-1.5357, -1.5014, -1.4672,  ..., -1.8953, -1.8953, -1.8782],\n",
      "           ...,\n",
      "           [ 1.7694,  1.8037,  1.8550,  ...,  1.8893,  1.8722,  1.8379],\n",
      "           [ 1.8037,  1.8208,  1.8550,  ...,  1.9064,  1.8893,  1.8550],\n",
      "           [ 1.8379,  1.8550,  1.8893,  ...,  1.9064,  1.9064,  1.8722]],\n",
      "\n",
      "          [[-1.5630, -1.5280, -1.3004,  ..., -2.0357, -2.0357, -2.0357],\n",
      "           [-1.7556, -1.7206, -1.5630,  ..., -2.0357, -2.0357, -2.0357],\n",
      "           [-1.8081, -1.7731, -1.7206,  ..., -2.0357, -2.0357, -2.0357],\n",
      "           ...,\n",
      "           [ 0.1527,  0.1877,  0.2052,  ...,  0.8529,  0.8354,  0.8004],\n",
      "           [ 0.1877,  0.2052,  0.2227,  ...,  0.8704,  0.8529,  0.8179],\n",
      "           [ 0.2227,  0.2402,  0.2577,  ...,  0.8704,  0.8704,  0.8354]],\n",
      "\n",
      "          [[-1.5779, -1.5953, -1.3861,  ..., -1.8044, -1.8044, -1.8044],\n",
      "           [-1.6999, -1.6824, -1.5430,  ..., -1.8044, -1.8044, -1.8044],\n",
      "           [-1.6824, -1.6476, -1.6127,  ..., -1.8044, -1.8044, -1.8044],\n",
      "           ...,\n",
      "           [-0.0092,  0.0256,  0.0605,  ...,  0.0605,  0.0953,  0.0779],\n",
      "           [ 0.0256,  0.0605,  0.0953,  ...,  0.0779,  0.0953,  0.0953],\n",
      "           [ 0.0953,  0.1128,  0.1302,  ...,  0.0605,  0.1128,  0.1128]]],\n",
      "\n",
      "\n",
      "         [[[-1.9124, -1.9124, -1.9124,  ..., -1.8268, -1.8268, -1.8439],\n",
      "           [-1.9124, -1.9124, -1.9295,  ..., -1.8268, -1.8439, -1.8610],\n",
      "           [-1.9295, -1.9295, -1.9295,  ..., -1.8439, -1.8610, -1.8782],\n",
      "           ...,\n",
      "           [-0.1657, -0.1657, -0.0972,  ..., -0.0116, -0.2342, -0.3027],\n",
      "           [-0.1999, -0.1828, -0.1143,  ...,  0.0056, -0.2171, -0.2684],\n",
      "           [-0.1828, -0.1314, -0.0629,  ...,  0.0056, -0.2171, -0.2684]],\n",
      "\n",
      "          [[-1.9482, -1.9482, -1.9482,  ..., -2.0357, -2.0357, -2.0357],\n",
      "           [-1.9482, -1.9482, -1.9657,  ..., -2.0357, -2.0357, -2.0357],\n",
      "           [-1.9657, -1.9657, -1.9657,  ..., -2.0182, -2.0182, -2.0357],\n",
      "           ...,\n",
      "           [-0.7227, -0.7402, -0.7577,  ..., -0.2150, -0.3901, -0.3725],\n",
      "           [-0.6877, -0.6877, -0.7052,  ..., -0.2150, -0.4076, -0.3725],\n",
      "           [-0.6702, -0.6702, -0.7052,  ..., -0.2325, -0.4076, -0.3725]],\n",
      "\n",
      "          [[-1.7522, -1.7522, -1.7347,  ..., -1.7696, -1.7696, -1.7870],\n",
      "           [-1.7522, -1.7522, -1.7522,  ..., -1.7522, -1.7696, -1.7870],\n",
      "           [-1.7696, -1.7696, -1.7522,  ..., -1.7522, -1.7696, -1.7870],\n",
      "           ...,\n",
      "           [-0.7936, -0.8807, -0.9156,  ..., -0.7413, -0.9156, -0.8981],\n",
      "           [-0.5495, -0.6367, -0.6890,  ..., -0.6018, -0.7587, -0.6890],\n",
      "           [-0.4275, -0.4450, -0.4798,  ..., -0.5844, -0.7064, -0.6367]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-1.0390, -1.0219, -1.0048,  ..., -1.2617, -1.2445, -1.2103],\n",
      "           [-1.0390, -1.0219, -1.0048,  ..., -1.2617, -1.2445, -1.2103],\n",
      "           [-1.0390, -1.0219, -1.0048,  ..., -1.2617, -1.2445, -1.2103],\n",
      "           ...,\n",
      "           [ 0.5022,  0.5364,  0.5536,  ..., -1.0733, -1.0904, -1.1075],\n",
      "           [ 0.5364,  0.5878,  0.6049,  ..., -1.0219, -1.0390, -1.0733],\n",
      "           [ 0.5536,  0.6049,  0.6221,  ..., -0.9877, -1.0048, -1.0390]],\n",
      "\n",
      "          [[-0.2675, -0.2850, -0.2850,  ..., -0.7227, -0.7052, -0.7227],\n",
      "           [-0.2675, -0.2850, -0.2850,  ..., -0.7227, -0.7052, -0.7227],\n",
      "           [-0.2675, -0.2850, -0.2850,  ..., -0.7227, -0.7052, -0.7227],\n",
      "           ...,\n",
      "           [ 0.3277,  0.3627,  0.3803,  ..., -0.8978, -0.9153, -0.9503],\n",
      "           [ 0.3627,  0.4153,  0.4328,  ..., -0.8452, -0.8627, -0.9153],\n",
      "           [ 0.3803,  0.4328,  0.4503,  ..., -0.8102, -0.8277, -0.8803]],\n",
      "\n",
      "          [[-0.7413, -0.7413, -0.7413,  ..., -1.0724, -1.0550, -1.0201],\n",
      "           [-0.7413, -0.7413, -0.7413,  ..., -1.0724, -1.0550, -1.0201],\n",
      "           [-0.7413, -0.7413, -0.7413,  ..., -1.0724, -1.0550, -1.0201],\n",
      "           ...,\n",
      "           [-0.2184, -0.1835, -0.1487,  ..., -0.9330, -0.9330, -0.9330],\n",
      "           [-0.1835, -0.1312, -0.0964,  ..., -0.8981, -0.8981, -0.8981],\n",
      "           [-0.1661, -0.1138, -0.0790,  ..., -0.8633, -0.8633, -0.8633]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3481,  0.3138,  0.2796,  ...,  0.0741,  0.0741,  0.0741],\n",
      "           [ 0.3481,  0.3138,  0.2796,  ...,  0.0741,  0.0741,  0.0912],\n",
      "           [ 0.3481,  0.3138,  0.2796,  ...,  0.0569,  0.0569,  0.0569],\n",
      "           ...,\n",
      "           [-0.3541, -0.3541, -0.3541,  ..., -1.4158, -1.4329, -1.3644],\n",
      "           [-0.3541, -0.3541, -0.3369,  ..., -1.2445, -1.3644, -1.3644],\n",
      "           [-0.3541, -0.3369, -0.3027,  ..., -1.0048, -1.3130, -1.4158]],\n",
      "\n",
      "          [[-0.7577, -0.7927, -0.8277,  ..., -0.8978, -0.8978, -0.8978],\n",
      "           [-0.7577, -0.7927, -0.8277,  ..., -0.9153, -0.9328, -0.9153],\n",
      "           [-0.7577, -0.7927, -0.8277,  ..., -0.9503, -0.9503, -0.9503],\n",
      "           ...,\n",
      "           [ 0.5553,  0.5553,  0.5553,  ..., -1.6155, -1.7731, -1.8782],\n",
      "           [ 0.5553,  0.5378,  0.5378,  ..., -1.4055, -1.6506, -1.8256],\n",
      "           [ 0.5553,  0.5553,  0.5553,  ..., -1.1429, -1.5455, -1.8081]],\n",
      "\n",
      "          [[-0.8284, -0.8633, -0.8981,  ..., -1.0898, -1.0724, -1.0550],\n",
      "           [-0.8284, -0.8633, -0.8981,  ..., -1.0898, -1.0724, -1.0550],\n",
      "           [-0.8284, -0.8633, -0.8981,  ..., -1.0898, -1.0724, -1.0550],\n",
      "           ...,\n",
      "           [ 0.0431,  0.0431,  0.0256,  ..., -1.6476, -1.6824, -1.6476],\n",
      "           [ 0.0605,  0.0256,  0.0082,  ..., -1.4384, -1.5953, -1.6302],\n",
      "           [ 0.0779,  0.0605,  0.0431,  ..., -1.2119, -1.5256, -1.6476]]],\n",
      "\n",
      "\n",
      "         [[[ 0.3994,  0.3994,  0.3994,  ...,  0.2453,  0.2282,  0.1768],\n",
      "           [ 0.3823,  0.3823,  0.3823,  ...,  0.2624,  0.2453,  0.1768],\n",
      "           [ 0.3652,  0.3652,  0.3652,  ...,  0.2967,  0.2624,  0.1768],\n",
      "           ...,\n",
      "           [-0.2171, -1.0904, -1.7754,  ..., -1.5185, -1.5185, -1.5014],\n",
      "           [-0.2171, -0.9020, -1.6213,  ..., -1.5185, -1.5185, -1.5014],\n",
      "           [-0.2513, -0.7308, -1.4843,  ..., -1.5185, -1.5185, -1.5014]],\n",
      "\n",
      "          [[-0.6352, -0.6176, -0.6352,  ..., -0.8102, -0.8102, -0.8277],\n",
      "           [-0.6527, -0.6527, -0.6527,  ..., -0.7927, -0.8102, -0.8277],\n",
      "           [-0.6702, -0.6702, -0.6702,  ..., -0.7577, -0.7752, -0.8277],\n",
      "           ...,\n",
      "           [ 0.3978, -0.6702, -1.5280,  ..., -1.8606, -1.8782, -1.8957],\n",
      "           [ 0.4153, -0.4776, -1.3880,  ..., -1.8606, -1.8782, -1.8957],\n",
      "           [ 0.4153, -0.3025, -1.2479,  ..., -1.8606, -1.8782, -1.8957]],\n",
      "\n",
      "          [[-0.8807, -0.8981, -0.9156,  ..., -0.9156, -0.9156, -0.9678],\n",
      "           [-0.8981, -0.8981, -0.9156,  ..., -0.8807, -0.8981, -0.9678],\n",
      "           [-0.9156, -0.9156, -0.9330,  ..., -0.8458, -0.8807, -0.9678],\n",
      "           ...,\n",
      "           [ 0.0256, -0.7413, -1.4210,  ..., -1.6650, -1.6476, -1.6302],\n",
      "           [ 0.0431, -0.5147, -1.2293,  ..., -1.6476, -1.6476, -1.6302],\n",
      "           [ 0.0431, -0.3230, -1.0724,  ..., -1.6476, -1.6476, -1.6302]]]]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoMAEImageProcessor {\n",
      "  \"_valid_processor_keys\": [\n",
      "    \"videos\",\n",
      "    \"do_resize\",\n",
      "    \"size\",\n",
      "    \"resample\",\n",
      "    \"do_center_crop\",\n",
      "    \"crop_size\",\n",
      "    \"do_rescale\",\n",
      "    \"rescale_factor\",\n",
      "    \"do_normalize\",\n",
      "    \"image_mean\",\n",
      "    \"image_std\",\n",
      "    \"return_tensors\",\n",
      "    \"data_format\",\n",
      "    \"input_data_format\"\n",
      "  ],\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"VideoMAEImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoMAEEmbeddings(\n",
      "  (patch_embeddings): VideoMAEPatchEmbeddings(\n",
      "    (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: tensor([[9.9980e-01, 1.8377e-04, 1.6858e-05]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('MobileCLIP-S1', pretrained='datacompdr')\n",
    "model.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "image = preprocess(Image.open(\"docs/CLIP.png\")).unsqueeze(0)\n",
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(model.visual.trunk.fork_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.visual.trunk.final_conv = torch.nn.Identity()\n",
    "model.visual.trunk.head = torch.nn.Identity()\n",
    "model.visual.head = torch.nn.Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = model.encode_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.6777e-01, -1.6147e-01, -1.6748e-01,  ...,  7.6636e-02,\n",
      "           -1.5139e-01, -1.1881e-01],\n",
      "          [-1.6004e-01, -1.6015e-01, -1.5510e-01,  ..., -1.1804e-01,\n",
      "           -1.6248e-01, -7.1789e-04],\n",
      "          [-1.6235e-01, -1.4091e-01, -1.5138e-01,  ..., -1.6069e-01,\n",
      "           -1.1205e-01, -1.6799e-01],\n",
      "          ...,\n",
      "          [-1.6934e-01, -1.5087e-01, -1.6184e-01,  ..., -1.2955e-01,\n",
      "           -8.4509e-02, -1.6715e-01],\n",
      "          [-1.6059e-01, -1.5737e-01, -9.9238e-02,  ..., -1.6912e-01,\n",
      "           -9.3838e-02, -1.5155e-01],\n",
      "          [-1.5996e-01, -7.4727e-02, -1.6797e-01,  ..., -1.6365e-01,\n",
      "           -1.6588e-01, -1.6966e-01]],\n",
      "\n",
      "         [[ 1.7520e-03,  6.2955e-03,  1.3091e-02,  ..., -5.2338e-02,\n",
      "            5.0242e-02,  4.2277e-02],\n",
      "          [-1.0292e-02,  1.3960e-02, -1.4933e-03,  ...,  3.7706e-02,\n",
      "            9.2762e-02,  2.4897e-01],\n",
      "          [ 1.0124e-02, -1.5115e-02,  1.9859e-02,  ...,  1.9876e-02,\n",
      "            1.5220e-01,  7.7530e-02],\n",
      "          ...,\n",
      "          [ 1.3186e-02,  4.7812e-02,  4.3653e-02,  ...,  2.5408e-02,\n",
      "            3.1201e-03, -1.0342e-02],\n",
      "          [ 3.7979e-02,  3.1208e-02,  5.0780e-02,  ...,  1.6840e-02,\n",
      "           -9.6105e-03,  1.2988e-02],\n",
      "          [ 3.2072e-02,  5.8023e-02,  7.7074e-03,  ..., -9.9440e-03,\n",
      "            1.1834e-02, -1.1209e-02]],\n",
      "\n",
      "         [[-6.8836e-02, -7.8096e-02, -7.7605e-02,  ...,  3.5332e-02,\n",
      "            1.1174e-01,  1.5205e-02],\n",
      "          [ 3.0591e-02, -5.1821e-02,  4.5943e-02,  ...,  1.2820e-01,\n",
      "            1.5613e-01, -8.2057e-02],\n",
      "          [ 4.4304e-02,  4.0484e-03,  1.1290e-01,  ...,  1.2330e-01,\n",
      "            1.9180e-01, -1.1699e-02],\n",
      "          ...,\n",
      "          [-5.7797e-02, -2.1388e-02, -4.1003e-02,  ...,  9.0369e-02,\n",
      "            3.6324e-03,  6.7607e-02],\n",
      "          [ 3.3057e-03,  9.9729e-03,  6.9926e-02,  ...,  1.4414e-02,\n",
      "            1.4051e-02,  1.1505e-02],\n",
      "          [-5.2815e-02, -1.0149e-02,  2.3120e-02,  ...,  6.9838e-03,\n",
      "           -4.8285e-02,  1.4906e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6760e-01, -1.6983e-01, -1.6631e-01,  ...,  1.8353e+00,\n",
      "           -1.3730e-02, -1.2426e-01],\n",
      "          [-1.0034e-01, -1.6618e-01, -1.5860e-01,  ..., -1.4637e-01,\n",
      "           -1.5978e-01, -9.5919e-02],\n",
      "          [-3.2488e-02, -1.5349e-01, -3.3607e-02,  ..., -1.3359e-01,\n",
      "           -1.6083e-01, -9.3941e-02],\n",
      "          ...,\n",
      "          [-7.9737e-02, -5.2107e-02, -1.6794e-01,  ...,  9.9033e-02,\n",
      "            8.6861e-01,  7.6452e-02],\n",
      "          [-1.3795e-01, -1.0358e-01, -9.6847e-02,  ...,  2.3557e-02,\n",
      "            6.6665e-01, -1.5529e-01],\n",
      "          [-9.8305e-02, -1.4090e-02,  6.5543e-02,  ..., -1.2175e-01,\n",
      "           -1.6874e-01, -6.6900e-02]],\n",
      "\n",
      "         [[-1.6440e-01, -1.6973e-01, -1.6932e-01,  ..., -1.1120e-01,\n",
      "           -9.0473e-02, -1.4451e-01],\n",
      "          [-9.5794e-02, -4.4995e-02, -8.7601e-02,  ...,  1.1944e-02,\n",
      "           -7.8188e-02,  1.6990e-01],\n",
      "          [-4.6450e-02,  1.2442e-01, -3.1784e-02,  ...,  4.4062e-02,\n",
      "           -7.0563e-02,  4.2580e-02],\n",
      "          ...,\n",
      "          [-8.5765e-03, -2.3188e-02, -7.2819e-02,  ..., -5.9927e-02,\n",
      "            1.8855e-01, -6.8865e-02],\n",
      "          [-9.5760e-02,  1.3326e-02,  3.6853e-02,  ..., -7.5648e-02,\n",
      "            5.3553e-01, -1.0325e-01],\n",
      "          [-1.4046e-01, -8.6409e-02, -3.6632e-02,  ..., -1.4558e-01,\n",
      "           -9.3825e-02, -5.5119e-02]],\n",
      "\n",
      "         [[ 1.4902e-03,  5.3826e-02,  1.3470e-02,  ..., -1.3916e-02,\n",
      "           -4.7194e-02, -3.3350e-02],\n",
      "          [ 3.0130e-03,  1.1249e-01,  3.9442e-02,  ..., -1.3912e-02,\n",
      "           -1.5962e-02, -3.0349e-02],\n",
      "          [-4.8357e-02, -4.7743e-02, -3.7617e-02,  ..., -9.9809e-03,\n",
      "           -3.2784e-02, -4.0646e-02],\n",
      "          ...,\n",
      "          [-4.8409e-02, -3.2326e-02, -1.6457e-03,  ..., -3.8752e-02,\n",
      "           -2.8303e-03, -7.4756e-02],\n",
      "          [-2.7945e-02, -3.6444e-05,  2.3783e-02,  ..., -2.4339e-02,\n",
      "           -1.4057e-02, -6.4351e-02],\n",
      "          [-3.8990e-02, -2.6793e-02, -2.2754e-02,  ..., -5.1747e-02,\n",
      "           -2.5697e-02, -8.1634e-02]]]])\n"
     ]
    }
   ],
   "source": [
    "print(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1024, 1024, 2, 3, 3], expected input[512, 16, 16, 14, 14] to have 1024 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 创建一些随机输入数据来测试模型\u001b[39;00m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)  \u001b[38;5;66;03m# B, num_frames, C, H, W 是输入数据的维度\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# 应该是 [B, num_frames/2, 1024, 14, 14]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mVideoFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39m(t, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m14\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# 使用 trilinear 模式适用于三维数据\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 时序特征融合\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openclip/lib/python3.10/site-packages/torch/nn/modules/conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[0;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1024, 1024, 2, 3, 3], expected input[512, 16, 16, 14, 14] to have 1024 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VideoFeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(VideoFeatureExtractor, self).__init__()\n",
    "        self.pretrained_model = pretrained_model  # 预训练的图像特征提取模型\n",
    "        self.conv3d = nn.Conv3d(in_channels=1024, out_channels=1024, kernel_size=(2, 3, 3), padding=(0, 1, 1), stride=(2, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, num_frames, C, H, W]\n",
    "        # 提取每一帧的特征\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = x.view(b * t, c, h, w)\n",
    "        x = self.pretrained_model(x)  # 假设预训练模型输出shape为[B*T, 1024, 8, 8]\n",
    "        \n",
    "        # 上采样特征图\n",
    "        x = x.view(b, t, 1024, 8, 8)\n",
    "        x = F.interpolate(x, size=(t, 14, 14), mode='trilinear', align_corners=False)  # 使用 trilinear 模式适用于三维数据\n",
    "        \n",
    "        # 时序特征融合\n",
    "        x = self.conv3d(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 假设你有一个预训练模型\n",
    "pretrained_model = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
    "model = VideoFeatureExtractor(pretrained_model)\n",
    "\n",
    "# 创建一些随机输入数据来测试模型\n",
    "x = torch.rand(512, 16, 1024, 8, 8)  # B, num_frames, C, H, W 是输入数据的维度\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)  # 应该是 [B, num_frames/2, 1024, 14, 14]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
